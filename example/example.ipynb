{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example starting from building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "from ase.data import atomic_numbers\n",
    "from nglview import show_ase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/nayoung/MOFFlow\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load building block files\n",
    "- Load building block files as ase.Atoms object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files\n",
    "import glob\n",
    "\n",
    "bb_files = glob.glob('example/*.xyz')\n",
    "\n",
    "# Load building block files as ase.Atoms object\n",
    "bb_atoms = []\n",
    "for file in bb_files:\n",
    "    bb_atoms.append(read(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3701259509340cabfabadb3f9bd734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize\n",
    "show_ase(bb_atoms[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare `data`\n",
    "- Goal: Align building blocks with pca axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equiv_vec(cart_coords, atom_types):\n",
    "    centroid = np.mean(cart_coords, axis=0)\n",
    "    weight = atom_types / atom_types.sum()\n",
    "    weighted_centroid = np.sum(cart_coords * weight[:, None], axis=0)\n",
    "    equiv_vec = weighted_centroid - centroid\n",
    "    if np.allclose(equiv_vec, 0):\n",
    "        dist = np.linalg.norm(cart_coords, axis=1)\n",
    "        sorted_indices = np.argsort(dist)\n",
    "        i = 0\n",
    "        while i < len(sorted_indices) and np.allclose(equiv_vec, 0):\n",
    "            equiv_vec = cart_coords[sorted_indices[i]]\n",
    "            i += 1\n",
    "    assert not np.allclose(equiv_vec, 0), \"Equivariant vector is zero\"\n",
    "    return equiv_vec\n",
    "\n",
    "def get_pca_axes(data):\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    centered_data = data - data_mean\n",
    "    covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "    if covariance_matrix.ndim == 0:\n",
    "        return np.zeros(3), np.eye(3)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def get_equivariant_axes(cart_coords, atom_types):\n",
    "    if cart_coords.shape[0] == 1:\n",
    "        return np.eye(3)\n",
    "    equiv_vec = get_equiv_vec(cart_coords, atom_types)\n",
    "    _, axes = get_pca_axes(cart_coords)\n",
    "    ve = equiv_vec @ axes\n",
    "    flips = ve < 0\n",
    "    axes = np.where(flips[None], -axes, axes)\n",
    "    right_hand = np.stack([\n",
    "        axes[:, 0], axes[:, 1], np.cross(axes[:, 0], axes[:, 1])\n",
    "    ], axis=1)\n",
    "    return right_hand\n",
    "\n",
    "def get_local_coords(bb_atoms):\n",
    "    local_coords_list = []\n",
    "    for bb in bb_atoms:\n",
    "        coords = bb.get_positions()\n",
    "        atom_types = np.array([atomic_numbers[s] for s in bb.get_chemical_symbols()])\n",
    "        center = coords.mean(axis=0)\n",
    "        centered_coords = coords - center\n",
    "        rotmat = get_equivariant_axes(centered_coords, atom_types)\n",
    "        local_coords = centered_coords @ rotmat\n",
    "        local_coords_list.append(local_coords)\n",
    "    return local_coords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute local_coords for each building block\n",
    "local_coords_list = get_local_coords(bb_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2780fed593274669ba33b92cd2e3d575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize aligned building blocks\n",
    "local_atoms = []\n",
    "\n",
    "for i, local_coords in enumerate(local_coords_list):\n",
    "    local_atoms.append(Atoms(\n",
    "        symbols=bb_atoms[i].get_chemical_symbols(),\n",
    "        positions=local_coords,\n",
    "        pbc=False\n",
    "    ))\n",
    "\n",
    "show_ase(local_atoms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data \n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# Concatenate number of building blocks\n",
    "bb_num_vec = torch.tensor([len(bb_atom) for bb_atom in bb_atoms]).int()\n",
    "\n",
    "# Concatenate atom types\n",
    "atom_types = torch.cat([torch.tensor(bb_atom.get_atomic_numbers()) for bb_atom in bb_atoms]).int()\n",
    "\n",
    "# Concatenate local_coords_list\n",
    "local_coords = torch.cat([torch.tensor(local_coords) for local_coords in local_coords_list]).float()\n",
    "\n",
    "# Create data, batch object\n",
    "data = Data(\n",
    "    num_nodes=bb_num_vec.shape[0],\n",
    "    num_atoms=local_coords.shape[0],\n",
    "    num_bbs=bb_num_vec.shape[0],\n",
    "    local_coords=local_coords,\n",
    "    atom_types=atom_types,\n",
    "    bb_num_vec=bb_num_vec,\n",
    ")\n",
    "batch = Batch.from_data_list([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nayoung/miniforge3/envs/mofflow-1/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from models.flow_module import FlowModule\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlowModule(\n",
       "  (model): FlowModel(\n",
       "    (node_feature_net): NodeFeatureNet(\n",
       "      (bb_embedder): Linear(in_features=64, out_features=128, bias=False)\n",
       "      (linear): Linear(in_features=384, out_features=256, bias=True)\n",
       "    )\n",
       "    (edge_feature_net): EdgeFeatureNet(\n",
       "      (linear_s_p): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (edge_embedder): Sequential(\n",
       "        (0): Linear(in_features=172, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (bb_embedder): BuildingBlockEmbedder(\n",
       "      (atom_type_embedder): Embedding(100, 64)\n",
       "      (edge_dist_embedder): GaussianSmearing()\n",
       "      (egnn_layers): ModuleList(\n",
       "        (0-3): 4 x E_GCL(\n",
       "          (edge_mlp): Sequential(\n",
       "            (0): Linear(in_features=193, out_features=64, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (3): ReLU()\n",
       "          )\n",
       "          (node_mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (trunk): ModuleDict(\n",
       "      (lattice_output): LatticeOutput(\n",
       "        (output): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mpa_0): MOFPointAttention(\n",
       "        (linear_q): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (linear_kv): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (linear_q_points): Linear(in_features=256, out_features=576, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=256, out_features=1440, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=24, bias=True)\n",
       "        (linear_l): Linear(in_features=6, out_features=24, bias=True)\n",
       "        (linear_out): Linear(in_features=2406, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mpa_ln_0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (tfmr_0): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (mha_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ffn_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (node_transition_0): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_0): BackboneUpdate(\n",
       "        (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "      (edge_transition_0): EdgeTransition(\n",
       "        (initial_embed): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (trunk): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (final_layer): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mpa_1): MOFPointAttention(\n",
       "        (linear_q): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (linear_kv): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (linear_q_points): Linear(in_features=256, out_features=576, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=256, out_features=1440, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=24, bias=True)\n",
       "        (linear_l): Linear(in_features=6, out_features=24, bias=True)\n",
       "        (linear_out): Linear(in_features=2406, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mpa_ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (tfmr_1): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (mha_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ffn_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (node_transition_1): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_1): BackboneUpdate(\n",
       "        (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "      (edge_transition_1): EdgeTransition(\n",
       "        (initial_embed): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (trunk): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (final_layer): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mpa_2): MOFPointAttention(\n",
       "        (linear_q): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (linear_kv): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (linear_q_points): Linear(in_features=256, out_features=576, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=256, out_features=1440, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=24, bias=True)\n",
       "        (linear_l): Linear(in_features=6, out_features=24, bias=True)\n",
       "        (linear_out): Linear(in_features=2406, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mpa_ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (tfmr_2): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (mha_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ffn_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (node_transition_2): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_2): BackboneUpdate(\n",
       "        (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "      (edge_transition_2): EdgeTransition(\n",
       "        (initial_embed): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (trunk): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (final_layer): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mpa_3): MOFPointAttention(\n",
       "        (linear_q): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (linear_kv): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (linear_q_points): Linear(in_features=256, out_features=576, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=256, out_features=1440, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=24, bias=True)\n",
       "        (linear_l): Linear(in_features=6, out_features=24, bias=True)\n",
       "        (linear_out): Linear(in_features=2406, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mpa_ln_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (tfmr_3): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (mha_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ffn_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (node_transition_3): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_3): BackboneUpdate(\n",
       "        (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "      (edge_transition_3): EdgeTransition(\n",
       "        (initial_embed): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (trunk): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (final_layer): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mpa_4): MOFPointAttention(\n",
       "        (linear_q): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (linear_kv): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (linear_q_points): Linear(in_features=256, out_features=576, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=256, out_features=1440, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=24, bias=True)\n",
       "        (linear_l): Linear(in_features=6, out_features=24, bias=True)\n",
       "        (linear_out): Linear(in_features=2406, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mpa_ln_4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (tfmr_4): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (mha_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ffn_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (node_transition_4): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_4): BackboneUpdate(\n",
       "        (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "      (edge_transition_4): EdgeTransition(\n",
       "        (initial_embed): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (trunk): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (final_layer): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mpa_5): MOFPointAttention(\n",
       "        (linear_q): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (linear_kv): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (linear_q_points): Linear(in_features=256, out_features=576, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=256, out_features=1440, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=24, bias=True)\n",
       "        (linear_l): Linear(in_features=6, out_features=24, bias=True)\n",
       "        (linear_out): Linear(in_features=2406, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mpa_ln_5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (tfmr_5): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (mha_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ffn_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_5): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (node_transition_5): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_5): BackboneUpdate(\n",
       "        (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load config\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name='inference.yaml')\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = \"logs/mof-csp/_published_batch_ver/ckpt/last.ckpt\" # TODO: change to your own path\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "flow_module = FlowModule.load_from_checkpoint(\n",
    "    checkpoint_path=ckpt_path,\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "flow_module.eval()\n",
    "flow_module.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.interpolant import Interpolant\n",
    "\n",
    "interpolant = Interpolant(cfg.interpolant)\n",
    "interpolant.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "batch = Batch.from_data_list([data])\n",
    "batch = batch.to(device)\n",
    "\n",
    "mof_traj, _ = interpolant.sample(\n",
    "    num_batch=batch.num_graphs,\n",
    "    num_bbs=batch.num_bbs,\n",
    "    model=flow_module.model,\n",
    "    atom_types=batch.atom_types,\n",
    "    local_coords=batch.local_coords,\n",
    "    batch_vec=batch.batch,\n",
    "    bb_num_vec=batch.bb_num_vec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core.lattice import Lattice\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from ase.build import make_supercell\n",
    "\n",
    "coords, lattice = mof_traj[-1]\n",
    "lattice = lattice.squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Make ase atoms\n",
    "mof = Structure(\n",
    "    lattice=Lattice.from_parameters(*lattice),\n",
    "    species=atom_types.detach().cpu().numpy(),\n",
    "    coords=coords.detach().cpu().numpy(),\n",
    "    coords_are_cartesian=True,\n",
    ")\n",
    "mof = AseAtomsAdaptor.get_atoms(mof)\n",
    "\n",
    "# Supercell\n",
    "multiplier = np.eye(3) * 2\n",
    "mof_supercell = make_supercell(mof, multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403aa29e5d614986be28afd082dd19a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize prediction\n",
    "viewer = show_ase(mof)\n",
    "viewer.add_unitcell()\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f9c614e00845249393bdb9c876d9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize supercell\n",
    "viewer = show_ase(mof_supercell)\n",
    "viewer.add_unitcell()\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mofflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
